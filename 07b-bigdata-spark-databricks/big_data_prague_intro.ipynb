{"cells":[{"cell_type":"markdown","source":["<img src='https://raw.githubusercontent.com/afo/dataXprague/master/imgs/dx_logo.png' width=600px></img>\n\n**Inspiriation and sources:** Databricks intro tutorial and [Using Apache Spark 2.0 to Analyze the City of San Francisco's Open Data](https://www.youtube.com/watch?v=K14plpZgy_c) by Sameer Farooqui\n\n# Introduction to Spark and Big Data\n\nDatabricks is a platform for running Spark without complex cluster management or tedious maintenance tasks. Spark is a distributed computation framework for executing code in parallel across many different machines. Databricks is the Spark team's enterprise solution makes big data simple by providing Spark as a hosted solution.\n\n## Databricks Terminology\n\n-   ****Workspaces**** : Where you store the ****notebooks**** and ****libraries****.\n-   ****Notebooks**** : Like Jupyter Notebooks that can run `Scala`, `Python`, `R`, `SQL`, or `Markdown`. Define language by `%[language name]` at the top of the cell. Connect to a cluster to run.\n-   ****Dashboards**** can be created from ****notebooks**** as a way of displaying the output of cells without the code.\n-   ****Libraries**** : Packages / Modules. You can install them via pypi.\n-   ****Tables**** : Structured data, that can be stored in data lake / cloud storage. Stored on Cluster or cached in memory.\n-   ****Clusters**** : Groups of computers that you treat as a single computer to perform operations on big sets of data.\n-   ****Jobs**** : Schedule execution on ****notebooks**** or Python scripts. They can be created either manually or via the REST API.\n-   ****Apps**** : 3rd party integrations with the Databricks platform like Tableau."],"metadata":{}},{"cell_type":"markdown","source":["### Spark's history\n\nSpark was developed by founders of Databricks in AMPLab at UC Berkeley. Started 2009, donated to Apache open source in 2013.\n\n### The Contexts/Environments\n\nBefore Spark 2.X many used the `sparkContext` made available as `sc` and the `SQLContext` made available as `sqlContext`. The `sqlContext` makes a lot of DataFrame functionality available while the `sparkContext` focuses more on the Apache Spark engine itself.\n\nIn Spark 2.X, there is just one context - the `SparkSession`.\n\n### The Data Interfaces\n\nKey interfaces.\n\n-   ****The DataFrame**** : Collection of distributed `Row` types (note no indicies for look up). Similar to pandas or R dataframe.\n-   ****The RDD (Resilient Distributed Dataset)**** : Interface to a sequence of data objects that consist of one or more types that are located across a variety of machines in a cluster. Focus on DataFrames as those will be supersets of the current RDD functionality.\n\nSee speed difference:\n\n<img src='https://databricks.com/wp-content/uploads/2015/02/Screen-Shot-2015-02-16-at-9.46.39-AM.png' width=600px></img>"],"metadata":{}},{"cell_type":"markdown","source":["Spark is a unified processing engine that can analyze big data using SQL, machine learning, graph processing or real time stream analysis. Streaming (infinte Dataframe), Machine Learning, Graph / Pagerank.\n\n![https://camo.githubusercontent.com/ed6aceb55bbc8761830b6effe52e8aa8ef146a99/687474703a2f2f637572726963756c756d2d72656c656173652e73332d776562736974652d75732d776573742d322e616d617a6f6e6177732e636f6d2f77696b692d626f6f6b2f626f6f6b5f696e74726f2f737061726b5f34656e67696e65732e706e67](https://camo.githubusercontent.com/ed6aceb55bbc8761830b6effe52e8aa8ef146a99/687474703a2f2f637572726963756c756d2d72656c656173652e73332d776562736974652d75732d776573742d322e616d617a6f6e6177732e636f6d2f77696b692d626f6f6b2f626f6f6b5f696e74726f2f737061726b5f34656e67696e65732e706e67)"],"metadata":{}},{"cell_type":"markdown","source":["You can read from many different data sources and Spark runs on every major environment. We will use Amazon EC2. We will read CSV data. Stick with Dataframe and SQL.\n\n![https://camo.githubusercontent.com/ed6aceb55bbc8761830b6effe52e8aa8ef146a99/687474703a2f2f637572726963756c756d2d72656c656173652e73332d776562736974652d75732d776573742d322e616d617a6f6e6177732e636f6d2f77696b692d626f6f6b2f626f6f6b5f696e74726f2f737061726b5f34656e67696e65732e706e67](https://camo.githubusercontent.com/165b53e995510bb3c5f77fc837d90faa9f222de6/687474703a2f2f637572726963756c756d2d72656c656173652e73332d776562736974652d75732d776573742d322e616d617a6f6e6177732e636f6d2f77696b692d626f6f6b2f626f6f6b5f696e74726f2f737061726b5f676f616c2e706e67)"],"metadata":{}},{"cell_type":"markdown","source":["# Let's Start\n\nBefore you start running code, you need to make sure that the notebook is attached to a cluster.\n\n### To create a Cluster\n\nClick the Clusters button that you'll notice on the left side of the page. On the Clusters page, click on ![img](https://training.databricks.com/databricks_guide/create_cluster.png) in the upper left corner.\n\nThen, on the Create Cluster dialog, enter the configuration for the new cluster.\n\nFinally, \n\n-   Select a unique name for the cluster.\n-   Select the most recent stable Runtime Version.\n-   Enter the number of workers to bring up - at least 1 is required to run Spark commands.\n\n\n**Go back to the notebook and in the top right corner press Detached and connect to your cluster.**\n\n*Note, Databricks community clusters only run for an hour*"],"metadata":{}},{"cell_type":"markdown","source":["first let's explore the previously mentioned `SparkSession` where info is stored. We can access it via the `spark` variable."],"metadata":{}},{"cell_type":"code","source":["spark"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["We can use the spark context to parallelize a small Python range that will provide a return type of `DataFrame`."],"metadata":{}},{"cell_type":"code","source":["firstDataFrame = spark.range(10000)\n\nprint(firstDataFrame) # if you just run a transformation no Spark Job is done."],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# or use RDD through sc (spark context)\nspark.sparkContext.parallelize(range(1000))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Now one might think that this would actually print the values parallelized. That's not how Spark works.\n\nSpark allows two distinct kinds of operations, **transformations** and **actions**.\n\n![ta2](https://camo.githubusercontent.com/f04cc9974c24d16f9425c4b86af1537cc9257dd0/687474703a2f2f637572726963756c756d2d72656c656173652e73332d776562736974652d75732d776573742d322e616d617a6f6e6177732e636f6d2f77696b692d626f6f6b2f67656e6572616c2f737061726b5f74612e706e67)\n\n### Transformations\n\nTransformations will only get executed once you have called a **action**. An example of a transformation might be to convert an integer into a float or to filter a set of values. I.e. Lazy Evaluation.\n\n### Actions\n\nActions are computed during execution. Run all of the previous transformations in order to get back an actual result. An action is composed of one or more jobs which consists of tasks that will be executed by the workers in parallel where possible.\n\nSshort sample of actions and transformations:\n\n![transformations and actions](https://training.databricks.com/databricks_guide/gentle_introduction/trans_and_actions.png)"],"metadata":{}},{"cell_type":"code","source":["firstDataFrame.show(3) # example of an action, dataframe is now evaluated"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# An example of a transformation\n# select the ID column values and multiply them by 2, SQL interfac\nsecondDataFrame = firstDataFrame.selectExpr(\"(id * 2) as value\")"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["secondDataFrame.show(5)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["from pyspark.sql.functions import col # to select columns\n\nfirstDataFrame.withColumn('id2', col('id')*2).show(3)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Or common before Spark 2.X\nfirstDataFrame.rdd.map(lambda x: x[0]*2).take(3)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# or\nfirstDataFrame.take(5)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# or\ndisplay(firstDataFrame)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Transformations are lazily evaluated because it is easy to optimize the entire pipeline of computations this way. Computations can be parallellized and executed on many different nodes at once (like a map and a filter).\n\n![transformations and actions](https://training.databricks.com/databricks_guide/gentle_introduction/pipeline.png)\n\nSpark also keeps results in memory, as opposed to other frameworks (e.g. Hadoop Map Reduce) that write to disk.\n\n## Spark Architecture\n\nSpark allows you to treat many machines as one via a master-worker architecture.\n\nThere is `driver` or master node in the cluster, accompanied by `worker` nodes. The master sends work to the workers and either instructs them to pull to data from memory or from disk (or from another data source).\n\nSpark Cluster has a Driver node that communicates with executor nodes. Executor nodes are logically like execution cores. \n\n![spark-architecture](https://training.databricks.com/databricks_guide/gentle_introduction/videoss_logo.png)\n\nThe Driver sends Tasks to the empty slots on the Executors when work has to be done:\n\n![spark-architecture](https://training.databricks.com/databricks_guide/gentle_introduction/spark_cluster_tasks.png)\n\nNote: In the case of the Community Edition there is no worker, the master executes the entire code. However, the same code works on any cluster (beware of CPU / GPU frameworks).\n\n![spark-architecture](https://docs.databricks.com/_static/images/notebooks/notebook-microcluster-agnostic.png)\n\nAccess details in the web UI by clicking at the top left of this notebook."],"metadata":{}},{"cell_type":"markdown","source":["# Working example with data\n\nTo illustrate **transformations** and **actions** - let's go through an example using `DataFrames` and a csv file of a public dataset that Databricks makes available. Available using the Databricks filesystem. Let's load the popular diamonds dataset in as a spark  `DataFrame`. Now let's go through the dataset that we'll be working with."],"metadata":{}},{"cell_type":"markdown","source":["Use `%fs` to interact with the spark filesystem"],"metadata":{}},{"cell_type":"code","source":["%fs ls /databricks-datasets/Rdatasets/data-001/datasets.csv"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["dataPath = \"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\"\ndiamonds = spark.read.format(\"csv\")\\\n  .option(\"header\",\"true\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(dataPath)\n  \n# inferSchema means we will automatically figure out column types \n# at a cost of reading the data more than once"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["Show the dataframe with Databricks `display` function or the show function."],"metadata":{}},{"cell_type":"code","source":["display(diamonds)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["display(diamonds.limit(5)) # for a subset"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["diamonds.printSchema() # see that the column types are OK and schema inferred correctly."],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["diamonds.rdd.getNumPartitions() # only one partition. This dataframe does not exist in memory. For big data several partitions.\n# Partitions can be optimized according to your cluster size. Have it divisible by cluster size.\n# For community edition, any number * 3 is OK\n# you can use REPARTITION method"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["diamonds.count() # reads through the whole data set"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["display(diamonds.summary())"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["diamonds.select('cut').distinct().show() # show unique entries in the cut column"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["What makes `display` exceptional is the fact that we can very easily create some more sophisticated graphs by clicking the graphing icon that you can see below. Here's a plot that allows us to compare price, color, and cut."],"metadata":{}},{"cell_type":"code","source":["display(diamonds)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# most common cut, ordered. First interesting insight.\ndisplay(diamonds.select('cut').groupBy('cut').count().orderBy('count',ascending=False))"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["display(diamonds.select('price','cut').groupBy('cut').avg('price')) # show graph, prepares 5 jobs"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["Now that we've explored the data, let's return to understanding **transformations** and **actions**. First transformations, then actions.\n\nFirst we group by two variables, cut and color and then compute the average price. Then we're going to inner join that to the original dataset on the column `color`. Then we'll select the average price as well as the carat."],"metadata":{}},{"cell_type":"code","source":["df1 = diamonds.groupBy(\"cut\", \"color\").avg(\"price\") # a simple grouping\n\ndf2 = df1\\\n  .join(diamonds, on='color', how='inner')\\\n  .select(\"`avg(price)`\", \"carat\")\n# a simple join and selecting some columns"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["These transformations are now complete in a sense but nothing has happened.\n\nThe reason for that is these computations are *lazy* in order to build up the entire flow of data from start to finish required by the user. This is an intelligent optimization for two key reasons. Any calculation can be recomputed from the very source data allowing Apache Spark to handle any failures that occur along the way, successfully handle stragglers. Secondly, Spark can optimize computation so that data and computation can be `pipelined`.\n\nTo get a sense for what this plan consists of, we can use the `explain` method."],"metadata":{}},{"cell_type":"code","source":["df2.explain()"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["Now explaining the above results is outside of this introductory tutorial. This is Spark's plan for how it hopes to execute the given query."],"metadata":{}},{"cell_type":"code","source":["df2.count()"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["This will execute the plan that Apache Spark built up previously. Click the little arrow next to where it says `(X) Spark Jobs` after that cell finishes executing and then click the `View` link. This brings up the Apache Spark Web UI right inside of your notebook.\n\n![img](https://training.databricks.com/databricks_guide/gentle_introduction/spark-dag-ui.png)\n\nThese are significant visualizations called Directed Acyclic Graphs (DAG)s of all the computations that have to be performed in order to get to that result. \n\nTransformations are *lazy* - while generating this series of steps Spark will optimize lots of things, one of core reasons that users should be focusing on using DataFrames and Datasets instead of the legacy RDD API. With DataFrames and Datasets, Apache Spark will work under the hood to optimize the entire query plan and pipeline entire steps together."],"metadata":{}},{"cell_type":"markdown","source":["# SQL view"],"metadata":{}},{"cell_type":"code","source":["diamonds.repartition(3).createOrReplaceTempView(\"diamondsView\") # also repartition, create a table view for SQL"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["diamonds.count()"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["%sql SELECT carat, cut, color from diamondsView ORDER BY carat DESC;"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["# in jupyter\nspark.sql('SELECT * FROM diamondsView').show()"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["# To pandas DataFrame"],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\n\npd_df = diamonds.toPandas()"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["pd_df.head(5)"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["type(pd_df)"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["### Caching\n\nSpark can store things in memory during computation. Can speed up access to commonly queried tables or pieces of data. This is also great for iterative algorithms that work over and over again on the same data.\n\nTo cache a DataFrame or RDD, simply use the cache method."],"metadata":{}},{"cell_type":"code","source":["df2.cache() # look in the UI / Storage"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["Caching, like a transformation, is performed lazily, won't store the data in memory until you call an action on that dataset. \n\nHere's a simple example. We've created our df2 DataFrame which is essentially a logical plan that tells us how to compute that exact DataFrame. We've told Apache Spark to cache that data after we compute it for the first time. So let's call a full scan of the data with a count twice. The first time, this will create the DataFrame, cache it in memory, then return the result. The second time, rather than recomputing that whole DataFrame, it will just hit the version that it has in memory.\n\nLet's take a look at how we can discover this."],"metadata":{}},{"cell_type":"code","source":["df2.count() # read all data and then materialize, cache it in memory\n# \n# Tungsten method to cache DataFrame into memory, makes it smaller.\n# Optimize by repartitioning according to your cluster also\n# Optimal partition sizes are 50-100Mb"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["However after we've now counted the data. We'll see that the explain ends up being quite different."],"metadata":{}},{"cell_type":"code","source":["df2.count()"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["In the above example, we can see that this cuts down on the time needed to generate this data immensely - often by at least an order of magnitude. With much larger and more complex data analysis, the gains that we get from caching can be even greater!"],"metadata":{}},{"cell_type":"code","source":["%fs ls /tmp/"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["# to save work and dataframe save as a Parquet file\ndiamonds.write.format('parquet').save('/tmp/diamonds/')"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["%fs ls /tmp/diamonds/"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["# Easily continue work if the cluster is shutdown, link to folder:\ndiamonds2 = spark.read.parquet('/tmp/diamonds/')"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["diamonds2.show() # will include all partitioning, cache into memory etc.\n\n# parque files are really efficient to read from. Always take CSV or JSON, do the ETL and then write to Parquet file."],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["## Conclusion\n\nIn this notebook we've covered a ton of material! But you're now well on your way to understanding Spark and Databricks!"],"metadata":{}}],"metadata":{"name":"big_data_prague","notebookId":1515220293376006},"nbformat":4,"nbformat_minor":0}
